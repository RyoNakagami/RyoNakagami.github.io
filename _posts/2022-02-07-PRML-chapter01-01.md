---
layout: post
title: "Bishop: パターン認識と機械学習の勉強用ノート 1/N"
subtitle: "Chapter 1: 多項式曲線Fitting"
author: "Ryo"
header-img: "img/bg-statistics.png"
header-mask: 0.4
catelog: true
mathjax: true
revise_date: 2022-03-15
tags:

- PRML
- Python
---


**Table of Contents**
<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

  - [はじめに](#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB)
  - [Bishop, PRML, 第一章ノート](#bishop-prml-%E7%AC%AC%E4%B8%80%E7%AB%A0%E3%83%8E%E3%83%BC%E3%83%88)
  - [Appendix](#appendix)
    - [演習問題1.1： 解答](#%E6%BC%94%E7%BF%92%E5%95%8F%E9%A1%8C11-%E8%A7%A3%E7%AD%94)
    - [演習問題1.2： 解答](#%E6%BC%94%E7%BF%92%E5%95%8F%E9%A1%8C12-%E8%A7%A3%E7%AD%94)
    - [演習問題1.3](#%E6%BC%94%E7%BF%92%E5%95%8F%E9%A1%8C13)
- [References](#references)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

### はじめに

- Bishop's book "Pattern Recognition and Machine Learning"の内容をベースにPythonを用いて本の主張を確認していくノートとなります
- ソースコードは[GitHub > RyoNakagami/PatternRecognitionMachineLearning](https://github.com/RyoNakagami/PatternRecognitionMachineLearning)
- ちょっと本の設定と異なるコードを回している箇所があります

### Bishop, PRML, 第一章ノート

<iframe src="https://nbviewer.org/github/RyoNakagami/PatternRecognitionMachineLearning/blob/main/python_jupyternotebooks/ch01-01-PolynomialCurveFitting.ipynb" width="100%" height="8400" frameborder="0"></iframe>


### Appendix
#### 演習問題1.1： 解答

Loss functionは以下,

$$
\begin{align*}
f(\mathbf w) &= \sum^{N}_{n=1} (X_n^T\mathbf w - t_n)^2\\
&= \sum^{N}_{n=1} (\sum_{j=0}^Mx_n^jw_j - t_n)^2 \tag{A-1}
\end{align*}
$$

(A-1)を$w_i$についてFOCを取ると、

$$
\begin{align*}
&\frac{\partial f}{\partial w_i}(\mathbf w)\\
&= \sum^{N}_{n=1}(\sum_{j=0}^Mx_n^jw_j - t_n)x_n^i\\
&\Rightarrow \sum^{N}_{n=1}x_n^i\sum_{j=0}^Mx_n^jw_j = \sum^{N}_{n=1}x_n^it_n \quad\quad\tag{A-2}
\end{align*}
$$

(A-2)より

$$
\sum_{j=0}^M(\sum^N_{n=1}x_n^{j+1})w_j = \sum^{N}_{n=1}x_n^it_n
$$

<div style="text-align: right;">
■
</div>

#### 演習問題1.2： 解答

考え方は演習問題1.1と同様だが(A-2)が以下のように変形されます

$$
\begin{align*}
&\frac{\partial f_{ridge}}{\partial w_i}(\mathbf w)\\
&= \sum^{N}_{n=1}(\sum_{j=0}^Mx_n^jw_j - t_n)x_n^i + \lambda w_i\\
&\Rightarrow \sum^{N}_{n=1}x_n^i\sum_{j=0}^Mx_n^jw_j + \lambda w_i = \sum^{N}_{n=1}x_n^it_n \quad\quad\tag{A-3}
\end{align*}
$$

(A-3)より

$$
\sum_{j=0}^M(\sum^N_{n=1}x_n^{j+1})w_j + \lambda w_i = \sum^{N}_{n=1}x_n^it_n
$$

$$
I_{ij} = \begin{cases}1 & \  \ \text{ where } i = j\\ 0 & \text{ otherwise }\end{cases}
$$

と定義すると

$$
\sum_{j=0}^M(\sum^N_{n=1}x_n^{j+1} + \lambda I_{ij})w_j = \sum^{N}_{n=1}x_n^it_n
$$

<div style="text-align: right;">
■
</div>

#### 演習問題1.3

選んだ箱を表す確率変数を $B = \{r, b, g\}$, 選んだフルーツを表す確率変数を $F = \{o, a, l\}$とします.

$$
\begin{align*}
Pr(F = a) &= \sum_{i \in \{r, b, g\}} Pr(B = i)Pr(F = a| B = i)\\
&= 0.3\times 0.2 + 0.5\times 0.2 + 0.3\times 0.6\\
&= 0.34
\end{align*}
$$

$$
\begin{align*}
Pr(B = g|F = o) &= \frac{Pr(F = o|B = g)Pr(B = g)}{Pr(F = o)}\\
&= \frac{0.3\times 0.6}{0.36}\\
&= 0.5
\end{align*}
$$

#### 演習問題1.5

とある分布に従う確率変数$x$の関数$f(x)$について、その期待値を$E[f(x)]$としたとき、いかが成立する：

<div class="math display" style="overflow: auto">
$$
\begin{align*}
var[f] &= E[(f(x) - E[f(x)])^2]\\
&= E[f(x)^2] + E[f(x)]^2 - 2E[f(x)]E[f(x)]\\
&= E[f(x)^2] - E[f(x)]^2
\end{align*}
$$
</div>






## References

- [GitHub > RyoNakagami/PatternRecognitionMachineLearning](https://github.com/RyoNakagami/PatternRecognitionMachineLearning)
- [GitHub > ctgk/PRML ](https://github.com/ctgk/PRML)